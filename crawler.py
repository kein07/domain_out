import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import re
from collections import deque

# --- è¨­å®šé …ç›® ---
INPUT_FILE = "domains.txt" 
OUTPUT_FILE = "found_cojp_links.txt"
MAX_PAGES_TO_CRAWL = 3000 # ä¸Šé™ã‚’å°‘ã—å¢—ã‚„ã—ã€æ™‚é–“å†…ã«çµ‚ã‚ã‚‹ã‹ãƒ†ã‚¹ãƒˆ
# å„ªå…ˆçš„ã«ã‚¯ãƒ­ãƒ¼ãƒ«ã—ãŸã„ãƒšãƒ¼ã‚¸ã®URLã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
PRIORITY_KEYWORDS = ['news', 'release', 'partner', 'policy', 'sitemap', 'company', 'about']
# --- è¨­å®šé …ç›®ã“ã“ã¾ã§ ---

visited_urls = set()
found_cojp_domains = set()

session = requests.Session()
adapter = requests.adapters.HTTPAdapter(max_retries=3)
session.mount('http://', adapter )
session.mount('https://', adapter )

def crawl_page(url, base_domain, normal_queue, priority_queue):
    if url in visited_urls:
        return
    
    print(f"[{len(visited_urls) + 1}/{MAX_PAGES_TO_CRAWL}] ğŸ” ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­: {url}")
    visited_urls.add(url)

    try:
        response = session.get(url, timeout=15, headers={'User-Agent': 'MyCoJpCrawler/1.0'})
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, "html.parser")

        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(url, href).split('#')[0] # URLã®#ä»¥é™ã¯ç„¡è¦–
            
            if not full_url.startswith('http' ):
                continue

            parsed_url = urlparse(full_url)
            domain = parsed_url.netloc

            # co.jpãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ç™ºè¦‹
            if domain.endswith('.co.jp'):
                if domain not in found_cojp_domains:
                    print(f"âœ¨ co.jpãƒ‰ãƒ¡ã‚¤ãƒ³ç™ºè¦‹: {domain}")
                    found_cojp_domains.add(domain)

            # åŒã˜ã‚µã‚¤ãƒˆå†…ã®æœªè¨ªå•ãƒšãƒ¼ã‚¸ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
            if domain == base_domain and full_url not in visited_urls:
                # å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€å„ªå…ˆã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
                if any(keyword in full_url for keyword in PRIORITY_KEYWORDS):
                    priority_queue.append(full_url)
                else:
                    normal_queue.append(full_url)
    except Exception as e:
        print(f"âš ï¸  ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•— or ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {url} ({e})")

def main():
    print("--- è³¢ã„ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™ ---")
    start_time = time.time()
    
    try:
        with open(INPUT_FILE, "r") as f:
            start_url = f.readline().strip()
        if not start_url:
            print("ã‚¨ãƒ©ãƒ¼: domains.txtãŒç©ºã§ã™ã€‚")
            return

        if not start_url.startswith('http' ):
            start_url = 'https://' + start_url
        
        base_domain = urlparse(start_url ).netloc
        
        # å„ªå…ˆã‚­ãƒ¥ãƒ¼ã¨é€šå¸¸ã‚­ãƒ¥ãƒ¼ã‚’ç”¨æ„
        priority_queue = deque()
        normal_queue = deque([start_url])

        print(f"å¯¾è±¡ã‚µã‚¤ãƒˆ: {start_url}")
        print(f"ã‚¯ãƒ­ãƒ¼ãƒ«ä¸Šé™: {MAX_PAGES_TO_CRAWL} ãƒšãƒ¼ã‚¸")
        print("--------------------")

        # ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œãƒ«ãƒ¼ãƒ—
        while (priority_queue or normal_queue) and len(visited_urls) < MAX_PAGES_TO_CRAWL:
            # å„ªå…ˆã‚­ãƒ¥ãƒ¼ã‚’å…ˆã«å‡¦ç†
            if priority_queue:
                current_url = priority_queue.popleft()
            else:
                current_url = normal_queue.popleft()
            
            crawl_page(current_url, base_domain, normal_queue, priority_queue)
            time.sleep(0.2) # å¾…æ©Ÿæ™‚é–“ã‚’å°‘ã—çŸ­ç¸®

        # çµæœã®æ›¸ãå‡ºã—
        if found_cojp_domains:
            print(f"\n--- çµæœ ---")
            print(f"{len(found_cojp_domains)}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªco.jpãƒ‰ãƒ¡ã‚¤ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
            with open(OUTPUT_FILE, "w") as f:
                f.write("\n".join(sorted(list(found_cojp_domains))))
            print(f"çµæœã‚’ {OUTPUT_FILE} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        else:
            print("\nco.jpãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®ãƒªãƒ³ã‚¯ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    except Exception as e:
        print(f"è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        end_time = time.time()
        print(f"\n--- ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº† ---")
        print(f"ç·å®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f} ç§’")
        print(f"è¨ªå•æ¸ˆã¿ãƒšãƒ¼ã‚¸æ•°: {len(visited_urls)}")

if __name__ == "__main__":
    main()
