import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import re

# --- è¨­å®šé …ç›® ---
INPUT_FILE = "domains.txt" 
OUTPUT_FILE = "found_cojp_links.txt"
# --- è¨­å®šé …ç›®ã“ã“ã¾ã§ ---

visited_urls = set()
found_cojp_domains = set()

# requestsã«ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã‚’æŒãŸã›ã‚‹ãŸã‚ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
session = requests.Session()
adapter = requests.adapters.HTTPAdapter(max_retries=3) # å¤±æ•—æ™‚ã«3å›ã¾ã§ãƒªãƒˆãƒ©ã‚¤
session.mount('http://', adapter )
session.mount('https://', adapter )


def crawl(url, base_domain):
    if url in visited_urls:
        return
    
    print(f"ğŸ” ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­: {url}")
    visited_urls.add(url)

    try:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’15ç§’ã«å»¶é•·ã—ã€ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§GET
        response = session.get(url, timeout=15, headers={'User-Agent': 'MyCoJpCrawler/1.0'})
        response.raise_for_status()
        
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, "html.parser")

        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(url, href)
            parsed_url = urlparse(full_url)
            domain = parsed_url.netloc

            if domain.endswith('.co.jp'):
                if domain not in found_cojp_domains:
                    print(f"âœ¨ co.jpãƒ‰ãƒ¡ã‚¤ãƒ³ç™ºè¦‹: {domain}")
                    found_cojp_domains.add(domain)

            if domain == base_domain:
                crawl(full_url.split('#')[0], base_domain)

    except requests.RequestException as e:
        print(f"âš ï¸  ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {url} ({e})")
    except Exception as e:
        print(f"ğŸ’¥ ä¸æ˜ãªã‚¨ãƒ©ãƒ¼: {url} ({e})")
    
    time.sleep(0.5)


def main():
    try:
        with open(INPUT_FILE, "r") as f:
            start_urls = [line.strip() for line in f if line.strip()]
        
        if not start_urls:
            print(f"ã‚¨ãƒ©ãƒ¼: {INPUT_FILE} ãŒç©ºã§ã™ã€‚")
            return
        
        start_url = start_urls[0]
        # httpsã‚’å„ªå…ˆçš„ã«è©¦ã™ã‚ˆã†ã«ä¿®æ­£
        if not start_url.startswith('http' ):
            start_url = 'https://' + start_url
        
        base_domain = urlparse(start_url ).netloc
        print(f"--- ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹ ---")
        print(f"å¯¾è±¡ã‚µã‚¤ãƒˆ: {start_url}")
        print(f"--------------------")

        crawl(start_url, base_domain)

        if found_cojp_domains:
            print(f"\n--- çµæœ ---")
            print(f"{len(found_cojp_domains)}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªco.jpãƒ‰ãƒ¡ã‚¤ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
            sorted_domains = sorted(list(found_cojp_domains))
            with open(OUTPUT_FILE, "w") as f:
                for domain in sorted_domains:
                    f.write(f"{domain}\n")
            print(f"çµæœã‚’ {OUTPUT_FILE} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        else:
            print("\nco.jpãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®ãƒªãƒ³ã‚¯ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    except FileNotFoundError:
        print(f"ã‚¨ãƒ©ãƒ¼: {INPUT_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    except Exception as e:
        print(f"è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
